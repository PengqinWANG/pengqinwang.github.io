<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Pengqin Wang</title>
  
  <meta name="author" content="Pengqin Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Pengqin Wang</name>
              </p>

              <p>
                I am a Ph.D. student in Robotics and Autonomous Systems(ROAS) 
                at <a href="https://hkust.edu.hk/">HKUST</a> and <a href="https://hkust-gz.edu.cn/">HKUST(GZ)</a>,
                under the supervision of <a href="https://uav.hkust.edu.hk/group/">Prof. Shaojie Shen</a> and <a href="https://meixinzhu.github.io/">Prof. Meixin Zhu</a>.
                My research interest is reinforcement learning and decision making.
              </p>

              <p>
                I received my B.Eng. degree in Electronic Information Engineering from <a href="https://www.sdu.edu.cn/">Shandong University</a> in 2021. 
                During my undergraduate study, I studied at Chongxin College and worked with <a href="https://faculty.sdu.edu.cn/zhoubin/zh_CN/index.htm">Prof. Bin Zhou</a>
                on autonomous navigation of ground robots.
              </p>

              <p style="text-align:center">
                [pwangas [at] connect (dot) ust (dot) hk]  <a href="https://scholar.google.com/citations?user=9l_mH8QAAAAJ&hl=zh-CN">[google scholar]</a>
              </p>
            </td>

          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/entropy.png" alt="clean-usnob" width="220" height="220">
            </td>
            <td width="75%" valign="middle">
              <papertitle>ENTROPY: Environment Transformer and Offline Policy Optimization</papertitle><br>
              <strong>Pengqin Wang</strong>, Meixin Zhu, Shaojie Shen<br>
              <p>
                Model-based methods provide an effective approach to offline reinforcement learning (RL). 
                They learn an environmental dynamics model from interaction experiences and then perform policy optimization based on the learned model. 
                However, previous model-based offline RL methods lack long-term prediction capability, resulting in large errors when generating multi-step trajectories. 
                We address this issue by developing a sequence modeling architecture, Environment Transformer, which can generate reliable long-horizon trajectories based on offline datasets. 
                We then propose a novel model-based offline RL algorithm, ENTROPY, that learns the dynamics model and reward function by ENvironment TRansformer and performs Offline PolicY optimization. We evaluate the proposed method on MuJoCo continuous control RL environments. 
                Results show that ENTROPY performs comparably or better than the state-of-the-art model-based and model-free offline RL methods and demonstrates more powerful long-term trajectory prediction capability compared to existing model-based offline methods.
              </p>
              <p>#Model_Based_RL  #Offline_RL  #Sequence_Modeling</p>
              <a href="https://arxiv.org/abs/2303.03811">[Paper]</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/new_catching.png" alt="clean-usnob" width="220" height="220">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Catch Planner: Catching High-Speed Targets in the Flight</papertitle>
              <br>
              Huan Yu, <strong>Pengqin Wang</strong>, Jin Wang, Jialin Ji, Zhi Zheng, Jie Tu, Guodong Lu, Jun Meng, Meixin Zhu, Shaojie Shen, Fei Gao<br>
              <p>
                Catching high-speed targets in the flight is a complex and typical highly dynamic task. In this paper, we propose Catch Planner, a planning-with-decision scheme for catching. 
                For sequential decision making, we propose a policy search method based on deep reinforcement learning. 
                In order to make catching adaptive and flexible, we propose a trajectory optimization method to jointly optimize the highly coupled catching time and terminal state while considering the dynamic feasibility and safety. 
                We also propose a flexible constraint transcription method to catch targets at any reasonable attitude and terminal position bias. 
                The proposed Catch Planner provides a new paradigm for the combination of learning and planning and is integrated on the quadrotor designed by ourselves, which runs at 100hz on the onboard computer. 
                Extensive experiments are carried out in real and simulated scenes to verify the robustness of the proposed method and its expansibility when facing a variety of high-speed flying targets.
              </p>
              <p>#Motion_Planning  #Decision_Making  #Spatio_Temporal_Trajectory_Optimization  #Deep_Reinforcement_Learning  #Catching</p>
              <a href="https://arxiv.org/abs/2302.04387">[Paper]</a>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last update: Mar 2023
                <br>
                <a href="https://github.com/jonbarron/website">Source</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
