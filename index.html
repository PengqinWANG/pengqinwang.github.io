<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Pengqin Wang</title>
  
  <meta name="author" content="Pengqin Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Pengqin Wang</name>
              </p>

              <p>
                I am a Ph.D. candidate in Robotics and Autonomous Systems(ROAS) 
                at <a href="https://hkust.edu.hk/">HKUST</a> and <a href="https://hkust-gz.edu.cn/">HKUST(GZ)</a>,
                under the supervision of <a href="https://uav.hkust.edu.hk/group/">Prof. Shaojie Shen</a> and <a href="https://meixinzhu.github.io/">Prof. Meixin Zhu</a>.
                My research interest involves reinforcement learning, decision making, autonomous driving and robotics.
              </p>

              <p>
                I received my B.Eng. degree in Electronic Information Engineering from <a href="https://www.sdu.edu.cn/">Shandong University</a> in 2021. 
                During my undergraduate study, I studied at Chongxin College and worked with <a href="https://faculty.sdu.edu.cn/zhoubin/zh_CN/index.htm">Prof. Bin Zhou</a>
                on autonomous navigation of ground robots.
              </p>

              <p style="text-align:center">
                [pwangas [at] connect (dot) ust (dot) hk]  <a href="https://scholar.google.com/citations?user=9l_mH8QAAAAJ&hl=zh-CN">[google scholar]</a>
              </p>
            </td>

          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
                <td style="padding:5px;width:2%;vertical-align:middle">
                </td>
                <td width="75%" valign="middle">
                  <p><strong>Traffic System Control and Simulation</strong></p>
                  <p>Teaching assistant, Fall 2022</p>
                  <a href="https://canvas.ust.hk/courses/46949">[course page]</a>
                  <p></p>
                </td>
              </tr>

        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/entropy.png" alt="clean-usnob" width="220" height="220">
            </td>
            <td width="75%" valign="middle">
              <papertitle>ENTROPY: Environment Transformer and Offline Policy Optimization</papertitle><br>
              <strong>Pengqin Wang</strong>, Meixin Zhu, Shaojie Shen<br>
              <p>
                Model-based methods provide an effective approach to offline reinforcement learning (RL). 
                They learn an environmental dynamics model from interaction experiences and then perform policy optimization based on the learned model. 
                However, previous model-based offline RL methods lack long-term prediction capability, resulting in large errors when generating multi-step trajectories. 
                We address this issue by developing a sequence modeling architecture, Environment Transformer, which can generate reliable long-horizon trajectories based on offline datasets. 
                We then propose a novel model-based offline RL algorithm, ENTROPY, that learns the dynamics model and reward function by ENvironment TRansformer and performs Offline PolicY optimization. We evaluate the proposed method on MuJoCo continuous control RL environments. 
                Results show that ENTROPY performs comparably or better than the state-of-the-art model-based and model-free offline RL methods and demonstrates more powerful long-term trajectory prediction capability compared to existing model-based offline methods.
              </p>
              <p>#Model_Based_RL  #Offline_RL  #Sequence_Modeling</p>
              <a href="https://arxiv.org/abs/2303.03811">[paper]</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/experiment_double.jpg" alt="clean-usnob" width="220" height="220">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Catch Planner: Catching High-Speed Targets in the Flight</papertitle>
              <br>
              Huan Yu<sup>*</sup>, <strong>Pengqin Wang<sup>*</sup></strong>, Jin Wang, Jialin Ji, Zhi Zheng, Jie Tu, Guodong Lu, Jun Meng, Meixin Zhu, Shaojie Shen, Fei Gao<br>
              <em><span style="color:#FF0000">IEEE Transactions on Mechatronics</span></em>, 2023, To appear.<br>
              <p>
                Catching high-speed targets in the flight is a complex and typical highly dynamic task. 
                However, existing methods require manual setting of catching height or time, resulting in lacks of adaptability and flexibility and cannot deal with multiple targets. 
                To bridge this gap, we propose a planning-with-decision scheme called Catch Planner. For sequential decision making, a lightweight policy search method based on deep reinforcement learning is proposed. 
                It is jointly trained with the motion planning and decoupled from physics to speed up training. For motion planning, we propose a trajectory optimization method that jointly optimizes the highly coupled catching time and terminal state. 
                The core is the flexible-terminal constraint transcription. It converts the three unique constraints of catching into differentiable metrics, including equality constraints for terminal position and time, and inequality constraints that enable reasonable terminal position offset and attitude relaxation. 
                In addition, sparse parameterization based on MINCO class considers both dynamic feasibility and collision avoidance constraints. As a result, a generally constrained quadrotor planning problem is transformed into an unconstrained optimization that can be solved reliably and efficiently. 
                We also propose an online iterative optimization method for predicting differentiable trajectories of targets. 
                Catch Planner provides a new paradigm for the combination of learning and planning, where all algorithms can be run in real time onboard at 100hz. 
                Extensive experiments are carried out in real-world and simulated scenes to verify the robustness and expansibility when facing a variety of high-speed flying targets.
              </p>
              <p>#Motion_Planning  #Decision_Making  #Spatio_Temporal_Trajectory_Optimization  #Deep_Reinforcement_Learning  #Catching</p>
              <a href="https://arxiv.org/abs/2302.04387">[paper]</a> <a href="https://www.youtube.com/watch?v=xrzVf31WsMM">[video]</a>
              <p></p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last update: June 2023
                <br>
                <a href="https://github.com/jonbarron/website">source</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
